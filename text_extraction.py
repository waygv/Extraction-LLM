# -*- coding: utf-8 -*-
"""text-extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-8Gm9L7VCYru0Mz9powmIQB51u1GKGq6
"""

!apt-get install -y tesseract-ocr
!pip install pytesseract

import pytesseract
pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"

!apt-get install poppler-utils
!pip install pdf2image

import os
import cv2
import numpy as np
from PIL import Image
import pytesseract
import matplotlib.pyplot as plt
from pdf2image import convert_from_path
from google.colab import files
import torch


# 1. Upload and Process PDF/Image

def upload_and_convert():
    """Allows user to upload an image or PDF and converts PDFs to images."""
    uploaded = files.upload()  # Opens file upload dialog

    if uploaded:
        file_name = list(uploaded.keys())[0]  # Get uploaded file name
        print(f"‚úÖ File uploaded: {file_name}")

        if file_name.lower().endswith(".pdf"):
            print("üìÑ Converting PDF to images...")
            images = convert_from_path(file_name)
            image_paths = []
            for i, img in enumerate(images):
                img_path = f"page_{i+1}.jpg"
                img.save(img_path, "JPEG")  # Save each PDF page as an image
                image_paths.append(img_path)
            return image_paths
        else:
            return [file_name]

    return None


# 2. Preprocess Image for OCR
def preprocess_image_for_ocr(image_path):
    """Preprocess image to enhance text for OCR."""
    image = cv2.imread(image_path)

    if image is None:
        print(f"‚ùå Error: Could not load image at path: {image_path}")
        return None


    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply adaptive thresholding for better contrast
    processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
    return processed


# 3. Extract Text Using OCR

def extract_text(image_path):
    """Extracts text from the image using Tesseract OCR."""
    processed_image = preprocess_image_for_ocr(image_path)

    if processed_image is None:
        return ""

    pil_image = Image.fromarray(processed_image)


    text = pytesseract.image_to_string(pil_image, config="--psm 6")
    return text.strip()

# 4. Draw Bounding Boxes (Optional)

def draw_bounding_boxes(image_path, save_path="output_with_boxes.jpg"):
    """Draw bounding boxes using Tesseract's text detection."""
    image = cv2.imread(image_path)

    if image is None:
        print(f"‚ùå Error: Could not load image at path: {image_path}")
        return

    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)

    for i in range(len(data['text'])):
        if int(data['conf'][i]) > 40:  # Confidence threshold
            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

    cv2.imwrite(save_path, image)


    plt.figure(figsize=(10, 6))
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.axis("off")
    plt.title("Detected Text Regions")
    plt.show()


# 5. Detection Pipeline
# ----------------------------
image_paths = upload_and_convert()

if image_paths:
    for img_path in image_paths:
        print(f"üîç Processing: {img_path}")

        # 1Ô∏è‚É£ Extract text from the image
        extracted_text = extract_text(img_path)

        # 2Ô∏è‚É£ (Optional) Draw bounding boxes
        draw_bounding_boxes(img_path)

        # 3Ô∏è‚É£ Display Extracted Text
        print("\nüìù **Extracted Text from Image:**\n")
        print(extracted_text)

!pip install transformers torch accelerate

from huggingface_hub import login
login("")# enter token here

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load Phi-2 Model
model_name = "microsoft/phi-2"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"üîÑ Loading model: {model_name} (This may take a minute)...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

print("‚úÖ Model loaded successfully!")

def generate_financial_story(text):
    """Pass extracted text through Phi-2 or any LLM to generate a financial history story."""
    prompt = f"""
    Below is text extracted from a company's financial report. Your job is to:
    1. Analyze and correct any spelling, grammar, and formatting issues.
    2. Construct a narrative that explains the company's financial history, including key trends, profit/loss analysis, and any notable financial events mentioned in the data.
    3. Write the narrative in a professional, engaging, and concise manner, avoiding technical jargon where possible. Make sure the story flows logically, starting from historical performance, and concluding with any forward-looking insights if the data suggests.

    --- Extracted Text ---
    {text}

    DO NOT provide JSON or any structured format.
    Only return a flowing story in paragraph format.
    """

    # Tokenize the prompt and run it through the LLM
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        output_tokens = model.generate(**inputs, max_new_tokens=1000)

    # Decode the LLM output into text
    financial_story = tokenizer.decode(output_tokens[0], skip_special_tokens=True).strip()

    return financial_story


# Example run of the financial story generation
if extracted_text:
    company_financial_story = generate_financial_story(extracted_text)
    print(company_financial_story)

